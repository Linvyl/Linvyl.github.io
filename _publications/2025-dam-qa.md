---
title: "Describe Anything Model for Visual Question Answering on Text-rich Images"
collection: publications
category: conferences
permalink: /publication/dam-qa-iccvw-2025
year: 2025
venue: "ICCV Workshops (VisionDocs)"
paperurl: https://openaccess.thecvf.com/content/ICCV2025W/VisionDocs/papers/Vu_Describe_Anything_Model_for_Visual_Question_Answering_on_Text-rich_Images_ICCVW_2025_paper.pdf
github: https://github.com/Linvyl/DAM-QA
citation: >
  Vu, Y.-L., Duong, D.-T., Duong, T.-B., Nguyen, A.-K., Nguyen, T.-H.,
  Nguyen, L. T. P., Xing, J., Li, X., Wang, T., Bagci, U., and Xu, M.
  Describe Anything Model for Visual Question Answering on Text-rich Images.
  In Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops (ICCVW), 2025.
---

**Abstract.**  
We introduce the Describe Anything Model (DAM), a vision–language framework for visual question answering on text-rich images. DAM integrates document understanding, OCR-aware representations, and multimodal reasoning to improve robustness on complex visual–textual inputs.
